# -------------------------------------------------------------------
# ðŸš€ LLMOps Business Idea Generator â€“ FastAPI Application
# -------------------------------------------------------------------
"""
This module defines the FastAPI application for the LLMOps Business Idea Generator.

The app provides a single endpoint (`/api`) that returns a new business idea
generated by an OpenAI model. This represents the initial deployable
version of the backend service, designed for quick integration and testing
with the Next.js frontend and Vercel deployment.
"""

# -------------------------------------------------------------------
# ðŸ“¦ Imports
# -------------------------------------------------------------------
from fastapi import FastAPI  # type: ignore
from fastapi.responses import PlainTextResponse  # type: ignore
from openai import OpenAI  # type: ignore


# -------------------------------------------------------------------
# âš™ï¸ Application Initialisation
# -------------------------------------------------------------------
app = FastAPI(
    title="LLMOps Business Idea Generator",
    description="A minimal FastAPI backend for generating AI-driven business ideas.",
    version="0.1.0",
)


# -------------------------------------------------------------------
# ðŸ’¡ Business Idea Generation Endpoint
# -------------------------------------------------------------------
@app.get("/api", response_class=PlainTextResponse)
def idea() -> str:
    """
    Generate a new business idea using an OpenAI chat model.

    This endpoint sends a simple user prompt to the model and
    returns the generated business idea as plain text. Designed
    for low-latency inference using `gpt-5-nano`.

    Returns
    -------
    str
        A new AI-generated business idea.
    """
    # Instantiate OpenAI client
    client = OpenAI()

    # Define prompt message for model input
    prompt = [
        {"role": "user", "content": "Come up with a new business idea for AI Agents"}
    ]

    # Create chat completion request using a lightweight model
    response = client.chat.completions.create(
        model="gpt-5-nano",
        messages=prompt
    )

    # Extract the generated idea text from the model response
    return response.choices[0].message.content
