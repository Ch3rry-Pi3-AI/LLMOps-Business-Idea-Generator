# 🚀 LLMOps – Business Idea Generator

### ⚡ Real-Time Streaming Branch

This branch enhances the **LLMOps Business Idea Generator** by introducing **real-time streaming** and **Markdown rendering**.
Instead of waiting for the full model response, users now see the idea appear live as the AI generates it — token by token — beautifully formatted using **React Markdown** and **Tailwind Typography**.

By the end of this stage, you will have a live, dynamic application where business ideas stream directly from OpenAI’s model to the frontend in real time.

## ⚙️ Overview

This stage introduces two key improvements:

* **Streaming API responses** from the FastAPI backend using `StreamingResponse`.
* **Dynamic frontend rendering** with `EventSource`, React state updates, and Markdown formatting.

The result is a smoother, more interactive user experience.

## 🪄 Step 1: Install Markdown Libraries

Install the required Markdown libraries for rendering rich text content:

```bash
npm install react-markdown remark-gfm remark-breaks
```

These packages allow your app to display formatted text such as bullet points, bold text, and line breaks generated by the AI.

## 🖥️ Step 2: Update the Frontend

Replace your existing `pages/index.tsx` with the following code:

```typescript
"use client"

import { useEffect, useState } from 'react';
import ReactMarkdown from 'react-markdown';
import remarkGfm from 'remark-gfm';
import remarkBreaks from 'remark-breaks';

export default function Home() {
    const [idea, setIdea] = useState<string>('…loading');

    useEffect(() => {
        const evt = new EventSource('/api');
        let buffer = '';

        evt.onmessage = (e) => {
            buffer += e.data;
            setIdea(buffer);
        };
        evt.onerror = () => {
            console.error('SSE error, closing');
            evt.close();
        };

        return () => { evt.close(); };
    }, []);

    return (
        <main className="p-8 font-sans">
            <h1 className="text-3xl font-bold mb-4">
                Business Idea Generator
            </h1>
            <div className="w-full max-w-2xl p-6 bg-white dark:bg-gray-800 border border-gray-300 dark:border-gray-600 rounded-lg shadow-md">
                <div className="prose prose-gray dark:prose-invert max-w-none">
                    <ReactMarkdown 
                        remarkPlugins={[remarkGfm, remarkBreaks]}
                    >
                        {idea}
                    </ReactMarkdown>
                </div>
            </div>
        </main>
    );
}
```

**Tailwind classes explained:**

* `prose`: Tailwind Typography plugin class for beautiful Markdown rendering
* `w-full max-w-2xl`: Full width with a max constraint for readable text
* `p-6`: Padding for balanced spacing
* `bg-white dark:bg-gray-800`: Adaptive light/dark background
* `border border-gray-300`: Subtle border separation
* `rounded-lg shadow-md`: Rounded corners and soft shadow for professional styling

**Note:** The `"use client"` directive remains essential — it ensures the component runs in the browser, allowing direct API calls to the FastAPI backend (rather than Next.js acting as a middle layer).

## 🎨 Step 3: Install Tailwind Typography Plugin

The `prose` class used above requires Tailwind’s official Typography plugin. Install it with:

```bash
npm install @tailwindcss/typography
```

This plugin enhances the readability of Markdown content and ensures consistent typography across themes.

## 🔄 Step 4: Update the Backend for Streaming

Replace your existing `api/index.py` with the following updated version:

```python
from fastapi import FastAPI  # type: ignore
from fastapi.responses import StreamingResponse  # type: ignore
from openai import OpenAI  # type: ignore

app = FastAPI()

@app.get("/api")
def idea():
    client = OpenAI()
    prompt = [{"role": "user", "content": "Come up with a new business idea for AI Agents"}]
    stream = client.chat.completions.create(model="gpt-5-nano", messages=prompt, stream=True)

    def event_stream():
        for chunk in stream:
            text = chunk.choices[0].delta.content
            if text:
                lines = text.split("\n")
                for line in lines:
                    yield f"data: {line}\n"
                yield "\n"

    return StreamingResponse(event_stream(), media_type="text/event-stream")
```

**What’s happening here:**

* The `stream=True` parameter enables **real-time token streaming** from the model.
* The `event_stream()` generator yields each chunk as it arrives.
* `StreamingResponse` sends data incrementally to the frontend using **Server-Sent Events (SSE)**.

## 🚀 Step 5: Test Streaming

Deploy and test your updated application:

```bash
vercel .
```

Visit the URL provided after deployment.
You’ll now see your **AI-generated business idea appear live on screen** — streaming word by word, beautifully formatted with Markdown.

## ✅ Completion Checklist

| Component            | Description                                           | Status |
| -------------------- | ----------------------------------------------------- | :----: |
| Streaming Backend    | FastAPI route streaming model output via SSE          |    ✅   |
| Live Frontend        | Next.js page rendering streamed text dynamically      |    ✅   |
| Markdown Rendering   | ReactMarkdown with GFM + soft breaks for clean output |    ✅   |
| Typography Plugin    | Installed Tailwind Typography for styled text         |    ✅   |
| Real-Time Experience | Ideas stream live from OpenAI to the user interface   |    ✅   |

## 🧭 Next Stage Preview → `03_styling`

The next branch (`03_styling`) will focus on **professional UI styling and visual polish**.
This includes refining the layout, adding dark mode adjustments, enhancing typography, and introducing a clean, modern interface for the **LLMOps Business Idea Generator**.
